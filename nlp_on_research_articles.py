"""
# -*- coding: utf-8 -*-
"""nlp-on-research-articles
Automatically generated by Colab.
Original file is located at:
https://colab.research.google.com/drive/11xh3zZFwQoaTflhnD70VMiX7g5_Q5LZr
"""

# ---------------------------
# 1. Import Libraries
# ---------------------------
import pandas as pd
import re
import string
import spacy
from tqdm import tqdm
import matplotlib.pyplot as plt
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split, cross_val_score, KFold
from sklearn.metrics import classification_report, accuracy_score
from sklearn.multioutput import MultiOutputClassifier
import numpy as np
import joblib

# ---------------------------
# 2. Load Data
# ---------------------------
# Training and test datasets
df_train = pd.read_csv('/kaggle/input/janatahack-independence-day-2020-ml-hackathon/train.csv')
df_test = pd.read_csv('/kaggle/input/janatahack-independence-day-2020-ml-hackathon/test.csv')

# Quick look at test data
df_test.head()
df_train.head()

# ---------------------------
# 3. Text Cleaning Function
# ---------------------------
def clean_text(text):
    """
    Clean input text by:
    - Lowercasing
    - Removing URLs
    - Removing unwanted punctuation (keep hyphens)
    - Removing extra whitespaces
    """
    text = text.lower()
    # Remove URLs
    text = re.sub(r'http\S+|www.\S+', '', text)
    # Remove punctuation except hyphen
    allowed_punct = "-–"
    punct_to_remove = ''.join([p for p in string.punctuation if p not in allowed_punct])
    text = re.sub(rf"[{re.escape(punct_to_remove)}]", " ", text)
    # Remove extra whitespaces
    text = re.sub(r'\s+', ' ', text).strip()
    return text

# Apply cleaning to title and abstract
df_train['TITLE'] = df_train['TITLE'].apply(clean_text)
df_train['ABSTRACT'] = df_train['ABSTRACT'].apply(clean_text)
df_test['TITLE'] = df_test['TITLE'].apply(clean_text)
df_test['ABSTRACT'] = df_test['ABSTRACT'].apply(clean_text)

# ---------------------------
# 4. Tokenization & Lemmatization using spaCy
# ---------------------------
nlp = spacy.load("en_core_web_sm", disable=["parser", "ner", "textcat"])

def spacy_token_lemma(texts):
    """
    Tokenize and lemmatize a list of texts using spaCy.
    Only alphabetic tokens are kept.
    """
    lemmas_list = []
    for doc in tqdm(nlp.pipe(texts, batch_size=100, n_process=1), total=len(texts)):
        lemmas = [token.lemma_ for token in doc if token.is_alpha]
        lemmas_list.append(" ".join(lemmas))
    return lemmas_list

# Apply tokenization & lemmatization
df_train['TITLE'] = spacy_token_lemma(df_train['TITLE'])
df_train['ABSTRACT'] = spacy_token_lemma(df_train['ABSTRACT'])
df_test['TITLE'] = spacy_token_lemma(df_test['TITLE'])
df_test['ABSTRACT'] = spacy_token_lemma(df_test['ABSTRACT'])

# ---------------------------
# 5. Combine Title and Abstract
# ---------------------------
df_train['TEXT'] = df_train['TITLE'] + ' ' + df_train['ABSTRACT']
df_test['TEXT'] = df_test['TITLE'] + ' ' + df_test['ABSTRACT']

# ---------------------------
# 6. Vectorization (TF-IDF)
# ---------------------------
vectorizer = TfidfVectorizer(max_features=10000)
X_train = vectorizer.fit_transform(df_train['TEXT'])
X_test = vectorizer.transform(df_test['TEXT'])

# ---------------------------
# 7. Define Target Columns
# ---------------------------
target_columns = ['Computer Science', 'Physics', 'Mathematics',
                  'Statistics', 'Quantitative Biology', 'Quantitative Finance']

# Explore label distribution
label_counts = df_train[target_columns].sum().sort_values(ascending=False)
print("Label counts:")
print(label_counts)

# Visualize label distribution
label_counts.plot(kind='bar', figsize=(10, 5), title='Label Distribution')
plt.ylabel('Number of samples')
plt.show()

# ---------------------------
# 8. Model Training
# ---------------------------
# Features and labels
y = df_train[target_columns]
X_text = df_train['TITLE'] + " " + df_train['ABSTRACT']
X = vectorizer.fit_transform(X_text)

# Initialize multi-label classifier
base_clf = LogisticRegression(max_iter=1000, class_weight='balanced')
multi_clf = MultiOutputClassifier(base_clf)

# 5-Fold Cross-Validation
cv = KFold(n_splits=5, shuffle=True, random_state=42)
print("Running 5-fold cross-validation...")
cv_scores = cross_val_score(multi_clf, X, y, cv=cv, scoring='accuracy')
print("Cross-validation accuracies:", cv_scores)
print("Mean accuracy:", np.mean(cv_scores))

# Train-validation split
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)

# Fit classifier
clf = MultiOutputClassifier(LogisticRegression(max_iter=1000, class_weight='balanced'))
clf.fit(X_train, y_train)

# ---------------------------
# 9. Evaluation
# ---------------------------
y_train_pred = clf.predict(X_train)
y_val_pred = clf.predict(X_val)

train_acc = accuracy_score(y_train, y_train_pred)
val_acc = accuracy_score(y_val, y_val_pred)

print("\nTrain accuracy:", train_acc)
print("Validation accuracy:", val_acc)

print("\nClassification report on validation set:\n")
print(classification_report(y_val, y_val_pred, target_names=target_columns))

# ---------------------------
# 10. Predictions on Test Set
# ---------------------------
y_test_pred = clf.predict(X_test)
y_test_pred_df = pd.DataFrame(y_test_pred, columns=y_train.columns)
y_test_pred_df.head()

# Save trained model
joblib.dump(clf, "multilabel_logreg_model.pkl")

# Save test predictions to CSV
y_test_pred_df.to_csv("test_predictions.csv", index=False)
print("\n✅ Predictions saved to 'test_predictions.csv'")
